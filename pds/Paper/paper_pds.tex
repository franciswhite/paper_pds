\documentclass{article}
\title{Philosophy of DS\\
\large Best Practices for the Application of Computational Methods to Questions of Meaning}
\date{}
\author{Silvan Hungerb{\"u}hler}
\usepackage{enumitem}
\usepackage{verbatim}
\usepackage{hyperref}
\usepackage{comment}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amsmath}

%\usepackage{biblatex}
%\addbibresource{pds_references.bib}
\usepackage{csquotes}
\begin{document}
\maketitle
\begin{abstract}
The increase in availability of data and advances in statistical methods has led to the use of Machine Learning (ML) technology in an immense variety of societal domains. ML algorithms are used to inform decisions as diverse and life-changing as access to financial credit, incarceration, university admission, medical treatment or the legal boundaries of espionage by the NSA.
The use of this technology is not always unproblematic, as its careless application can cause serious harm, even if such effects occur unintentionally.
The seriousness of the potential to disrespect people's privacy and integrity or systematically disadvantage specific groups has led to a recent debate concerning ethical guidelines and best practices for the general use of ML.
Little has been said, however, about the proper use of algorithms in the specific field of Distributional Semantics (DS). Some researchers have called attention to possible dangers of using ML in DS, as it harbours the potential to reinforce harmful biases along dimensions of race and gender.
The present paper wants to point to one particular attempt to provide general-purpose guidelines for ML and draw on these existing insights to suggest guidelines and best practices for computational methods in DS.
As the domain of DS differs in important aspects from other applications of ML, notably also for natural language processing,
this involves assessing  the applicability of general purpose ML guidelines to DS with these difference in mind.
\end{abstract}
\section{Introduction}
%1-2 paragraphs, talk about application, overall field
As vast amounts of digital data have become available over past decades, algorithms are employed to use the data in order to take and inform decisions all across society. 
Algorithms have a say in determining who is granted mortgages, who is released on bail or invited for job applications \cite{algorithms2016}.
The theory behind many of these algorithms is \emph{machine learning} (ML).
Although most inventors and designers of ML algorithms certainly do not harbor malicious intent, the risk of unwillingly harming people is real.

There are several different sources of harmful effects that these techniques can bring about. With the very large data collected, stored and processed, issues of privacy arise naturally. Especially when data mining techniques are used to scrape the web, people's explicit consent to provide their data can easily go by the board. Another recurrent problem is the two-sided nature of any technology: it can be used for the good or for dubious purposes, as fraud or spying, for example. Finally, big data and ML approaches brings the risk of reenforcing structural bias along racial, gender or others social lines in society. Models trained on biased historical data, for example, have been used to determine prison sentences which lead to consistently biased outcomes against blacks and latinos in the United States of America \cite{angwin2016machine}.

%largest part of intro, preview of paper, methodology!
Clearly, many of these issues are of great importance to modern society at large. Consequently, people working at the intersection of information technology and ethics have attempted to lay down foundations on how to handle these technologies. The realization that algorithms fed on big data come with a host of problematic issues has led to the emergence of a variety of principles, concrete guidelines and public as well as institutional scrutiny. Many of these efforts have naturally been directed at the broad topic of ML in general.\cite{leidner2017ehtical} Only a few years ago the discussion has turned on the specific use of ML for analysis and modeling of language - sometimes broadly referred to as \emph{natural language processing} (NLP). This is only one of many particular fields where ML is applied and the discussion of ethics in ML naturally seep through from more general to more domain-specific applications. 

A recently as 2016, however, a discussion has opened up in a hitherto completely undiscussed domain. ML techniques used for the computer-based study of language meaning - a field known as \emph{computational DS} - has come under scrutiny.
Various studies have shown that these methods can  yield results which are in important ways biased against particular groups as, for example, women \cite{google} \cite{wagner2015s}.
\textbf{Why this is important. Mention Google searches using semantic information.} 

In the present paper we want to draw on the existing efforts made by experts in the fields of ethics, computer science, public policy and law to extend principles  and guidelines for general purpose ML ethics to the field of DS. Specifically, we will use the \emph{Principles for Accountable Algorithms} (PAA) issued by the organization \emph{Fairness , Accountability and Transparency in Machine Learning} (FATML) - a consortium of researches and experts from private and public organizations working on ML. 
The PAA comprise five guiding principles: responsibility, explainability, accuracy, auditibility and fairness \cite{principles}.
We will assess potential violation of techniques used in DS for each of these principles and, where applicable, issue guidelines to prevent their negative impact upon vulnerable groups. 
The present contribution works by scanning previous efforts made in a similar vein, identifying useful parts therein and applying them to ethical questions surrounding DS. We will make no foundational efforts to sustain the validity of the principles. They are presupposed. Our analysis merely serves to discuss the usefulness of the principles in particular domain, and assert it where due.

Currently, we want to focus on aspects of discrimination and bias in the outcome of DS models. This for two reasons: For one, these are the least explored ethical issues for DS. Many of the ideas, guidelines and principles concerning privacy, double use or economic impact are directly transferable from the general ML debate to any sub-field that similarly employs big data and algorithms. Computational DS is no exception. Debates concerning privacy etc. can follow the same lines as the discussion to establish general purpose ethical principles for ML. The issue of discrimination and bias, however, has a special relation to DS because the field occupies a unique position of computational language research. The central concern of DS, as its name gives away, is the concept of meaning. As we will argue presently, this relates it in a special way to many sensitive topics in the general ML discussion. Put succinctly, it sits at the intersection of concerns pertaining to data selection, validity or questionability of computational methods, interpretation of results and usage thereof.

The paper is structured as follows: In \hyperlink{sec2}{Section 2} we first give a very brief description of ML, NLP and DS as well as their relation to each other. Especially for DS we want to provide overview over societal domains where such technology is currently applied and could conceivably be applied in the future. \hyperlink{sec3}{Section 3} contains a discussion of the important ways in which DS, as a particular application of big data analysis, differs from other instances of ML. For the present paper these particularities are important insofar as they entail differences when it comes to its risks and accountabilities across the spectrum of ML. In \hyperlink{sec4}{Section 4}, then, we try to apply the \emph{Principles for Accountable Algorithms} by FATML to the specific case of DS. \hyperlink{sec5}{Section 5} contains concluding remarks while \hyperlink{sec6}{Section 6} lists the references.
\section{Machine Learning, Natural Language Processing and Distributional Semantics}\hypertarget{sec2}{ }
The three terms that make up the title of this section are frequently muddled in literature pertaining to what this paper is about. In order to avoid any confusion, and give people foreign to any such field a quick overview, the following paragraphs contain a short description of the three terms and their mutual relation.
 
The term \emph{machine learning} is an umbrella term for a variety of statistical methods that are able to \emph{learn} from a series of examples presented to them. The set of examples - often called the \emph{training data} - is not simply memorized, but the ML algorithm extracts generalizable knowledge like structures, patterns or relationships \cite{domingos2012few}.
The result of the learning process is a statistical description - or \emph{model} - of the training data \cite{fayyad2001digital}.

\emph{Natural language processing} is a field of research that uses computational tools to build natural language text or speech applications. It is related to computational linguistic, an area of linguistic that uses computational techniques to get a scientific understanding of natural language. The knowledge thus obtained can be used  to develop computer systems that serve a wide variety of purposes. Principal applications include translation between natural languages, artificial intelligence, user interfaces, like chat-bots, for example, summarization of text or speech recognition \cite{chowdhury2003natural}. The area of NLP frequently serves itself of methods from ML, for example to process very large textual corpora. Indeed, parts of NLP can be thought of as particular applications of ML techniques to the domain of natural language.

\emph{Distributional semantics} is a linguistic theory of meaning looking to represent lexical meaning of expressions as a function of the context in which they appear. Its theoretically foundation is the so-called \emph{distributional hypothesis} which holds that two expressions are similar in meaning, if they occur in similar contexts \cite{harris1954distributional}.

The theory of DS has been used successfully in computational semantics - a branch of computational lingiustics. This recent success is attributable to increased availability of large amounts of digitalised language data and ML techniqeus. Particularly, it has become possible to obtain semantic representations immediately from natural language data. The theoretical framework of DS has become widely used and further developed as the advent and success of computational methods have enabled linguist to build large-scale models of linguistic meaning using digitalised corpora of text. 

Following the distributional hypothesis, the meaning of an expression is a function of the context in which it occurs, where context can be construed in different ways. Most typically, it is other words surrounding an expression in a text - the sentence in which it appears, for example. Context is not restricted to such simple analysis, however, as it can possibly contain more complex syntactic relations within a text, or even visual, textual or auditory information gathered from data.
\cite{boleda2016formal}
DS, then, takes the context in which an expression occurs and abstracts away from the particular contexts in a large corpus of natural language data. The mathematical vehicle for representing these abstractions are commonly vectors, but there are also other algebraic objects possible, as matrices or tensors. The meaning of an expression, as a function of its context, is then represented by the distribution across the dimensions of the vector. A set of words thus represented builds a semantic space - mathematically, a vector space - wherein one can study the semantic relationship between the expressions because some semantic relationships between expressions are mirrored by geometric relationship in these spaces.

DS is concerned with the specific topic of meaning in the study of natural language. If viewed from an application-oriented standpoint it could be seen as one of many sub-fields of NLP research. Indeed, as far as the present paper is concerned, it is precisely the application of DS in industry that warrants heightened attention.
For conreteness' sake, consider a situation where a meaning space is constructed based on some seemingly innocuous corpus and ends up containing severe bias along lines of gender or racial stereotypes. A recent study has found, for example, that a model trained on Google News articles displays significant bias along binary gender dimensions \cite{bolukbasi2016man}. This raises a host of questions concerning the provenance of the bias, the adequate response by the responsible distributional semanticist and, not to mention, the nature of meaning.

Does one have to live with the fact that the meanings discovered in some corpus systematically contain unfavourable connotations for some group of people? After all, the corpus was produced by a particular society, so it should be expected that it mirror that society's prejudices. Is the stance "the model picked up what it is supposed to pick up, blame society" a fair one, or has whoever works with the model the obligation to rid it of biased meaning representations? The questions go on: If the semantic model is further used for some interaction within that same society that taught it - via corpus - what it means to be a \emph{woman}, will the bias be further re-enforced? It is has been reported that when one queries Google with the name of a person, then Google is more likely to show advertisement related to that person being arrested if the name's connotation is black \cite{sweeney2013discrimination}.

\section{Ethics of Machine Learning}
\paragraph{The state of the ethical discussion in ML, NLP and DS}
For the domain general use of ML techniques there exists a series of studies by from legal, political computer science experts which specify principles and guidelines. Furthermore, many non-technical articles have been published outside of academic channels. 
be specific, mention FATML. \\
\textbf{use nlp ethic article to get overview. do this shits.}

In the case of  there has been  has seen some movement in the last couple of years.
For ML and  there are several  conferences on ethical issues by now.
http://www.ethicsinnlp.org/related-conferences

 even concrete proposals for best practices.\\
-DS in particular has seen little attention.

\paragraph{What good should come from looking at DS?}
-focus not on dual-use or bad treatment of mechanical turks, but the question how bias in meaning is dangerous.\\
-Why not ethics by design
A first concern is the issue of privacy. Since very large amounts of data are required for these methods to be effective, great efforts are devoted to collecting large data sets which may contain highly sensitive information about individuals or organizations. It is thus of importance who gets to see these data and how they are protected from third parties accessing them. There have been reported cases of researchers working with large corpora of texts containing peoples' complete medical record who sent these files indiscriminately around by email. A second aspect of privacy is how the data is collected in the first place. It is a question of ethical concern whether the individuals whose data is collected agree to their personal information being used. What the standards should be here isstill  far from clear as automated data-mining techniques gain popularity.

A second point concerns the disruptive potential ML techniques have on labor markets. As ever more cognitive tasks can be automated, the economic landscape of service sector economies is bound undergo change. Some workers will without a doubt be disadvantaged by these changes. It must concern the inventors and implentors of such technology what happens here.

A third difficult poit is the so-called \emph{double use} problem. New technology's use is often underspecified and could be used for benefit of humanity or to its detriment. Chat-bots developed by researchers in automated , to name a concrete example, can be used to assist people to access governmental health-care serices, yet they have reportedly been used by the Mexican government to disrupt political discussions deemed subversive on social media. \cite{leidner2017ethical}

Finally, ML harbours the potential to amplify structural discrimination and systematically disadvantage people because of their race, gender or other social category. If the models which are trained algorithmically are fed biased data, then predictions or classifications they make will reflect that bias.
Data processed by ML techniques often contains societal prejudice and bias, so that unquestioningly using them for decision making can severely reinforce society's existing biases. A related issue is that data available to the algorithms may systematically omit information relevant to what it is trying to uncover, as when minority groups are not adequately represented in the data. Yet another issue is that ML techniques may use useful patterns and relationships in the data that are really mirrorring historical factors of discrimination and marginalization. \cite{barocas2016big}

\section{Principles for  for Machine Learning}\hypertarget{sec3}{}
Algorithmically informed decisions are used in ever more domains of the private and public sectors and thus have the potential to impact society in significant ways. In many cases of undesirable effects that come with these technologies, legislation is behind in adequately protecting people negatively affected by it.

Given this state of affairs, it is important to hold the people who develop or implement such algorithmic systems accountable to the public. The FATML concisely write in their PAA \cite{principles}
\begin{quote}
Algorithms and the data that drive them are designed and created by people -- There is always a human ultimately responsible for decisions made or informed by an algorithm. ''The algorithm did it'' is not an acceptable excuse if algorithmic systems make mistakes or have undesired consequences, including from machine-learning processes.
\end{quote}

The document then goes on to name five principles that should guide developders of algorithms in designing and implementing their systems. The authors of PAA urge creators to issue a Social Impact Statement wherein they detail the effect of their product has on society with respect to the five principles. In this way the principles are associated with a set of best practices.

We will now briefly go over the five principles - \emph{responsibility, explanability, accuracy, auditability and fairness} to describe their original intention in the PAA.
Responsibility requires developers to take seriously the possibility that adverse effects are produced their algorithmic system. Consequently, there should be designated roles within the organization to be responsible for redress if harm is caused. Excuses akin to ''the algorithm did it'' are no valid scapegoats. This reflects the acknowledgment that, at some level, there are people who design these systems and put them to use.
Explainability demands that both data selection and decisions taken by the algorithms can be explained to people affected by its usage in terms that are understandable by laywomen. The point here is not to obscure the workings of the systems to a point where no stakeholder can grasp it.
Accuracy asks designers and users of the system to keep track of its mistakes and sources of error, so that worst case scenarios become more easily anticipited and possible damage reduced.
Audibility is the requirement to create transparency in such a way that interested third parties are in the position to review the algorithms performance. This point is related to explainability.
Finally, Fairness requires that the algorithmic system does not affect different demographic groups in disparate and unfavourable ways, that is, discriminates against them.

It should be fair to say that some of these principles allow seamless transfer from general ML to any specific subdomain, as DS, without much further discussion.
This goes for the principles corresponding to accuracy - loggin mistakes is good practice no matter where -, auditibility - one should never obstruct uninterested parties' inquiries into the possibility of things going wrong - and explainability - obfuscating one's work and keeping affected parties ignorant is evidently bad practice both in ML and in DS.
Much more interesting than the overlap lately mentioned, however, is the question how the two remaining principles apply to to DS.

Concerning the question of responsibility, consider a concrete scenario where the problem arises: Google trains a DS model on a corpus of news articles on Google News. The model contains semantic representations of lexical word meaning. Despite the articles in the corpus being written by professional journalists, these meanings contain clear bias along a binary gender dimension. One of many striking examples is the fact that the word \emph{woman} bears the same semantic relation to \emph{homemaker} in the meaning space as the word \emph{man} to \emph{computer scientist}. Now, a search engine like Google could use semantic information available in such a model, with bias and all, to optimize the relevance of their search results. It is quite conceivable that an employer queries Google for ''Computer Scientist Kabul'' to find professionals in the area and Google deems Linkedin profiles with male-associated names more relevant to the search request. Consequently, male computer scientists will find their profiles ranked higher in the results of the query and thus obtain more visibility. This would be a clear-cut case of discrimination and disadvantage based on gender, brought about directly by recommendation mechanism which utilizes DS.

Who is to blame for this unfortunate outcome? The recommendation systems designers might simply point at the corpus in which they found their word meanings. After all, the corpus presumably reflects the fact that there are men than women who work in computer science accurately \cite{womenincs}. So the biased outcome of the search mirrors societal structure at, or so would the recommender's excuse sound. 

A further point to consider is the fact that the nature of the object supposedly captured by DS - meaning - is highly contested. Conceptual confusion and debate abound concerning the exact relation around the concepts of meaning, groundedness, speaker and community. This can make it even harder to adequately respond to calls of responsibility in these matters because the relation that holds between corpus, DS techniques, meaning and society are far from settled. 

This little story goes to show that it can be excrutiatingly difficult to find the responsible human agent when DS is used for algorithmic systems. Even if one grants that the meanings as found in a corpus are biased in an unacceptable way, the question still remains what acceptability would entail. A follow to this is whether one can remove prejudice and bias from meaning spaces without rendering them unusable for usage in applications. What, for example, if the fact that the expression "woman" is functional in natural language discourse precisely because of its unfortunate semantic relationship to man. A recent paper by Bolukbasi et al. contradicts this pessimistic view \cite{bolukbasi2016man}. According to it, it is possible to debias the meanings of a semantic space along binary gender dimenions while still preserving its useful properties.

The lesson of this particular successfull debiasing endeavour is straightforward. Performance is no general excuse for designers of systems employing DS to work with meanings that reflect societal bias. Efforts can be made to process the DS model after its original training to rid it of discriminatory disparaties. In general domain ML there is already a sizeable body of literature of finding an removing so-called \emph{disparate impact} - unintentional discrimination (cf. \cite{feldman2015certifying} \cite{dwork2012fairness}). 
Similar collections of standardized techniques should be developed to debias semantic spaces. Doing this is especially important, of course, if the DS model to be used for some outside application. 
Perhaps a complement to the attempts to debiase embeddings, then, are the studies whose stated goal it is to uncover biases in corpora, as eg. \cite{wagner2015s} or \cite{herbelot2012distributional}. Findings that indicate clear instances of bias in widely used corpora can then be used as an indication where  debiasing should have to take place before the DS is put to further use.
Thus, it appears that a two-fold effort is required in to prevent damage from happening in the employment of algorithmically generated meanings. On the one hand, one will constantly have to scan the semantic spaces gathered from corpora for undesired connotations or other semantic relationships within it; on the other, once such biases are certified, efforts need to be undertaken, so that systems using this semantic information do not systematically disadvantage people.

In this fashion fairness principle could be addressed in DS. This does not yet respond, however, to the question where exactly responsibility lies - that is, with whom - in case something does go wrong. 

\section{Conclusion}\hypertarget{sec x}{}

\section{References}\hypertarget{sec5}{ }
%\printbibliography

\end{document}