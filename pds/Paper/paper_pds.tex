\documentclass{article}
\title{Practical Ethics for Computational Semantics\\
\large Philosophy of Distributional Semantics}
\date{}
\author{Silvan Hungerb{\"u}hler}
\usepackage{enumitem}
\usepackage{verbatim}
\usepackage{hyperref}
\usepackage{comment}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amsmath}
\usepackage{biblatex}
\addbibresource{pds_references.bib}
\usepackage{csquotes}
\begin{document}
\maketitle
\begin{abstract}
The increase in availability of data and advances in statistical methods has led to the use of Machine Learning (ML) technology in an immense variety of societal domains. ML algorithms are used to inform decisions as diverse and life-changing as access to financial credit, incarceration, university admission, medical treatment or the legal boundaries of espionage by the NSA.
The use of this technology is not always unproblematic, as its careless application can cause serious harm, even if such effects occur unintentionally.
The seriousness of the potential to disrespect people's privacy and integrity or systematically disadvantage specific groups has led to a recent debate concerning ethical guidelines and best practices for the general use of ML.
Little has been said, however, about the proper use of algorithms in the specific field of Distributional Semantics (DS). Some researchers have called attention to possible dangers of using ML in DS, as it harbors the potential to reinforce harmful biases along dimensions of race and gender.
The present paper wants to point to one particular attempt to provide general-purpose guidelines for ML and draw on these existing insights to suggest guidelines and best practices for computational methods in DS.
As the domain of DS differs in important aspects from other applications of ML, notably also natural language processing,
this involves assessing  the applicability of general purpose ML guidelines to DS with these difference in mind.
\end{abstract}
\section{Introduction}
As vast amounts of digital data have become available over past decades, algorithms are employed to use the data in order to take and inform decisions all across society. 
Algorithms have a say in determining who is granted mortgages, who is released on bail or invited for job applications \cite{algorithms2016}.
The theory behind many of these algorithms is \emph{machine learning} (ML).
Although most inventors and designers of ML algorithms certainly do not harbor malicious intent, the risk of unwillingly harming people is real.

There are several different sources of harmful effects that these techniques can bring about. With the very large amounts of data collected, stored and processed, issues of privacy arise naturally. Especially when sophisticated data gathering techniques are used to scrape the web, peoples's explicit consent to provide their data can easily go by the board. Another recurrent problem is the two-sided nature of any technology: it can be used for the good or for dubious purposes, as fraud or spying, for example. Finally, big data and ML approaches harbor the risk of re-enforcing structures of discrimination along racial, gender or others social lines. In a widely publicized case, to name one concrete example, automated systems have been used for legal decisions which lead to consistently biased outcomes against Blacks and Latinos in the USA \cite{angwin2016machine}.

Many of these issues are of great importance to modern societies. Consequently, people working at the intersection of information technology and ethics have attempted to lay down foundations on how to handle these technologies. The realization that algorithms fed on big data come with a host of problematic issues has led increased public as well as institutional scrutiny (notably, the oval office \cite{united2014big}) and, consequently, to the elaboration of a variety of principles and concrete guidelines for their use. Many of these efforts have naturally been directed at the broad topic of ML in general \cite{leidner2017ethical}. Only a few years ago the discussion has turned on the specific use of ML for analysis and modeling of language - sometimes broadly referred to as \emph{natural language processing} (NLP).

As recently as 2016, however, a discussion has opened up in a hitherto completely undiscussed domain. ML techniques used for the computer-based study of language meaning - a field known as \emph{computational distributional semantics} - has come under scrutiny.
Various studies have demonstrated that these methods can  yield results which are in important ways biased against particular groups as, for example, women \cite{bolukbasi2016man}\cite{wagner2015s}.
This form of machine bias in word meaning can affect people in discriminatory ways. It is quite conceivable that in the near future many applications, for example automated customer services, will operate with automatically generated representations of natural language meaning. If semantic bias translates into discriminatory decisions being taken, then there is a problem.

In the present paper we want to draw on the existing efforts made by experts in the fields of ethics, computer science, public policy and law to extend principles  and guidelines for general purpose ML ethics to the field of distributional semantics (DS). Specifically, we will use the \emph{Principles for Accountable Algorithms} (PAA) issued by the organization \emph{Fairness , Accountability and Transparency in Machine Learning} (FATML) - a consortium of researches and experts from private and public organizations working on ML. 
The PAA comprise five guiding principles: responsibility, explainability, accuracy, auditability and fairness \cite{principles}.
We will assess in what ways techniques used in DS could potentially violate these principles. This then allows us to issue guidelines to prevent their negative impact upon vulnerable groups. 

The present contribution works by scanning previous efforts made in a similar vein, identifying useful parts therein and applying them to ethical questions surrounding DS. We will make no foundational efforts to sustain the validity of the principles. They are presupposed. Our analysis merely serves to discuss the usefulness of the principles in particular domain, and assert them where due.
Currently, we want to focus on aspects of discrimination and bias in the outcome of DS. This is for two reasons: For one, these are the least explored ethical issues for DS. Furthermore, some of the ideas, guidelines and principles concerning privacy, abuse of otherwise legitimate technology ot  or economic impact are directly transferable from the general ML debate to any sub-field that similarly employs big data and algorithms. Computational DS is no exception. Debates concerning privacy etc. can follow the same lines as the discussion to establish general purpose ethical principles for ML. The issue of discrimination and bias, however, has a special relation to DS because the field occupies a unique position of computational language research. The central concern of DS, as its name gives away, is the concept of meaning. As we will argue presently, this relates it in a special way to many sensitive topics in the general ML discussion. Put succinctly, it sits at the intersection of concerns pertaining to data selection, validity or questionableness of computational methods, interpretation of its results and the use it is given in applications.

The paper is structured as follows: In \hyperlink{sec2}{Section 2} we first give a very brief description of ML, NLP and DS as well as their relation to each other. Also contained in this section is an assessment of the extent in which ehtical discussions are already led in this field. \hyperlink{sec3}{Section 3} sketches in what ways ML is susceptible to harm people and introduces the PAA. This forms the backdrop for \hyperlink{sec4}{Section 4} where we assess in what precise way this specific set of ethical principles designed for ML is applicable to DS. \hyperlink{sec5}{Section 5} contains concluding remarks and \hyperlink{sec6}{Section 6} lists the references.

\section{ML, NLP and DS}\hypertarget{sec2}{ }
The three abbreviations that make up the title of this section are frequently muddled in related literature. In order to avoid confusion, and give people alien to any such field a quick overview, the following paragraphs contain a short description of the three terms and their relation.
 
The term \emph{machine learning} is an umbrella term for a variety of statistical methods that are able to \emph{learn} from a series of examples presented to them. The set of examples - often called the \emph{training data} - is not simply memorized, but the ML algorithm extracts generalizable knowledge like structures, patterns or relationships \cite{domingos2012few}.
The result of the learning process is a statistical description - or \emph{model} - of the training data \cite{fayyad2001digital}.

\emph{Natural language processing} is a field of research that uses computational tools to build natural language text or speech applications. It is related to computational linguistic, an area of linguistic that uses computational techniques to get a scientific understanding of natural language. The knowledge thus obtained can be used  to develop computer systems that serve a wide variety of purposes. Principal applications include translation between natural languages, artificial intelligence, user interfaces, like chat-bots, for example, summarization of text or speech recognition \cite{chowdhury2003natural}. The area of NLP frequently serves itself of methods from ML, for example to process very large textual corpora. Indeed, parts of NLP can be thought of as particular applications of ML techniques to the domain of natural language.

\emph{Distributional semantics} is a linguistic theory of meaning looking to represent lexical meaning of expressions as a function of the context in which they appear. Its theoretically foundation is the so-called \emph{distributional hypothesis} which holds that two expressions are similar in meaning, if they occur in similar contexts \cite{harris1954distributional}.

The theory of DS has been used successfully in computational semantics - a branch of computational lingiustics. This recent success is attributable to increased availability of large amounts of digitalized language data and ML techniques. Particularly, it has become possible to obtain semantic representations immediately from natural language data. The theoretical framework of DS has become widely used and further developed as the advent and success of computational methods have enabled linguist to build large-scale models of linguistic meaning using digitalised corpora of text. 

Following the distributional hypothesis, the meaning of an expression is a function of the context in which it occurs, where context can be construed in different ways. Most typically, it is other words surrounding an expression in a text - the sentence in which it appears, for example. Context is not restricted to such simple analysis, however, as it can possibly contain more complex syntactic relations within a text, or even visual, textual or auditory information gathered from data. \textbf{Moar refs?}

DS, then, takes the contexts in which an expression occurs in a large corpus of natural language data and abstracts away from the particular contexts. The mathematical vehicle for representing these abstractions are commonly vectors, but there are also other algebraic objects possible, as matrices or tensors. The meaning of an expression, as a function of its context, is then represented by the distribution across the dimensions of the vector. A set of words thus represented builds a semantic space - mathematically, a vector space - wherein one can study the semantic relationship between the expressions because some semantic relationships between expressions are mirrored by geometric relationship in these spaces \cite{boleda2016formal}.

DS is concerned with the specific topic of meaning in the study of natural language. If viewed from an application-oriented standpoint it could be seen as one of many sub-fields of NLP research. Indeed, as far as the present paper is concerned, it is precisely the application of DS in industry that requires heightened attention.


\section{Principles for  for Machine Learning}\hypertarget{sec3}{}
Algorithmically informed decisions are used in ever more domains of the private and public sectors and thus have the potential to impact society in significant ways. We now specify four ways in which unreflected usage of ML can bring about harm.

A first concern is the issue of privacy. Since very large amounts of data are required for these methods to be effective, great efforts are devoted to collecting large data sets which may contain highly sensitive information about individuals or organizations. It is thus of importance who gets to see these data and how they are protected from third parties accessing them. There have been reported cases of researchers working with large corpora of texts containing peoples' complete medical record who sent these files indiscriminately around by email. A second aspect of privacy is how the data is collected in the first place. It is a question of ethical concern whether the individuals whose data is collected agree to their personal information being used.

A second point concerns the disruptive potential ML techniques have on labor markets. As ever more cognitive tasks can be automated, the economic landscape of service sector economies is bound to undergo big change. Some workers will without a doubt be disadvantaged by these changes. 

A third difficult point is the so-called \emph{double use} problem. New technology's use is often underspecified and could be used for benefit of humanity or to its detriment. Chat-bots developed by researchers in NLP, to name a concrete example, can be used to assist people to access governmental health-care services, yet they have reportedly been used by the Mexican government to disrupt political discussions deemed subversive on social media \cite{leidner2017ethical}.

Finally, ML harbors the potential to amplify structural discrimination and systematically disadvantage people because of their race, gender or other social category. If the models which are trained algorithmically are fed biased data, then predictions or classifications they make will reflect that bias.
Data processed by ML techniques can reflect societal prejudice and bias, so that unquestioningly using them for decision making can reinforce society's existing discriminatory structure. A related issue is that data available to the algorithms may systematically omit information relevant to what it is trying to uncover, as when minority groups are not adequately represented in the data. Yet another issue is that ML techniques may use useful patterns and relationships in the data that are really mirroring historical factors of discrimination and marginalization \cite{barocas2016big}.


In the broader field of ML some people seem to have recognized the importance ethical considerations. For the domain-general use of ML techniques there exists a series of studies by from legal, political computer science experts which specify principles and guidelines (cf. \cite{pasquale2015black}\cite{feldman2015certifying}\cite{barocas2016big}. Starting in 2014 with a workshop series at the \emph{Conference on Neural Information Processing Systems}, an organization called \emph{Fairness, Accountability, and Transparency in Machine Learning} (FATML) has been working on solutions for ethical problems in ML. Furthermore, many non-technical articles have been published outside of academic channels. 

In the case of NLP there has been  has seen some movement in the last couple of years. There have been discussion regarding the moral agency of chat-bots \cite{thieltges2016devil}, as well as more general contributions to ethical guidelines in broader NLP \cite{hovy2016social}.
Additionally, both for ML and NLP many public events were hosted and conferences established to discuss ethical issues (cf. \cite{conferences}).
As a field, however, DS in particular has seen little attention in this respect. Although there are already proposals for best practices in NLP \cite{leidner2017ethical}, for example, no similar effort has been made for DS.

In many cases of undesirable effects that come with these technologies, legislation is behind in adequately protecting people negatively affected by it.
Given this state of affairs, it is important to hold the people who develop or implement such algorithmic systems accountable to the public. The FATML concisely write in their PAA\cite{principles}: 
\begin{quote}
Algorithms and the data that drive them are designed and created by people -- There is always a human ultimately responsible for decisions made or informed by an algorithm. ''The algorithm did it'' is not an acceptable excuse if algorithmic systems make mistakes or have undesired consequences, including from machine-learning processes.
\end{quote}

The document then goes on to name five principles that should guide developers of algorithms in designing and implementing their systems. The authors of PAA urge creators to issue a Social Impact Statement wherein they detail the effect of their product has on society with respect to the five principles. In this way the principles are associated with a set of best practices.

We will now briefly go over the five principles - \emph{responsibility, explainability, accuracy, auditability and fairness} to describe their original intention in the PAA.
Responsibility requires developers to take seriously the possibility that adverse effects are produced their algorithmic system. Consequently, there should be designated roles within the organization to be responsible for redress if harm is caused. Excuses akin to ''the algorithm did it'' are no valid scapegoats. This reflects the acknowledgment that, at some level, there are people who design these systems and put them to use.
Explainability demands that both data selection and decisions taken by the algorithms can be explained to people affected by its usage in terms that are understandable by laywomen. The point here is not to obscure the workings of the systems to a point where no stakeholder can grasp it.
Accuracy asks designers and users of the system to keep track of its mistakes and sources of error, so that worst case scenarios become more easily anticipated and possible damage reduced.
Audibility is the requirement to create transparency in such a way that interested third parties are in the position to review the algorithms performance. This point is related to explainability.
Finally, Fairness requires that the algorithmic system does not affect different demographic groups in disparate and unfavorable ways, that is, discriminates against them.
We now want to assess to what degree we can use these principles for the field of DS.
\section{Principles for Distributional Semantics}\hypertarget{sec4}{}
It should be fair to say that some of these principles allow seamless transfer from general ML to any specific sub-domain, as DS, without much further discussion.
This goes for the principles corresponding to accuracy - loging mistakes is good practice no matter where -, auditability - one should never obstruct uninterested parties' inquiries into the possibility of things going wrong - and explainability - obfuscating one's work and keeping affected parties ignorant is evidently bad practice both in ML and in DS.
Much more interesting than the overlap lately mentioned, however, is the question how the two remaining principles apply to to DS.

Concerning the question of fairness, consider a concrete scenario where the problem arises: A recent study has found, for example, that a model of lexical word meaning trained on Google News articles displays significant bias along binary gender dimensions \cite{bolukbasi2016man}. Despite the articles in the corpus being written by professional journalists, the word \emph{woman} bears the same semantic relation to \emph{homemaker} in the meaning space as the word \emph{man} to \emph{computer scientist}. A search engine could use semantic information available in such a model, with bias and all, to optimize the relevance of their search results. Imagine an employer querying Google for ''Computer Scientist Kabul'' to find professionals in the area and Google deems LinkedIn profiles with male-associated names more relevant to the search request. Consequently, male computer scientists will find their profiles ranked higher in the results of the query and thus obtain more visibility. This would be a clear-cut case of discrimination and disadvantage based on gender, brought about directly by recommendation mechanism which utilizes DS. These are not mere hypotheticals though, but already very concrete issues. It is has been reported that when one queries Google with the name of a person, then Google is more likely to show advertisement related to that person's criminal record if the name is associated with black people \cite{sweeney2013discrimination}.

This raises a host of questions concerning the provenance of the bias in the model, the adequate response by the responsible distributional semanticist and, not to mention, the nature of meaning.
Does one have to live with the fact that the meanings discovered in some corpus systematically contain unfavorable connotations for some group of people? After all, the corpus was produced by a particular society, so it should be expected that it mirror that society's prejudices. What position should one assume towards these results? Is it fair to claim that the the model picked up what it is supposed to pick up and blame society for systematically biased meanings? Or does whoever works with the model the obligation to rid it of biased meaning representations? Such questions gain concrete importance if the semantic model is further used for some interaction within that same society that taught it - via corpus - what it means to be a \emph{woman}, for example. 
What if using the model results in re-enforcement of the structures that lead to the biased corpus in the first place? 
Who is to blame for this unfortunate outcome? The recommendation systems designers might simply point at the corpus in which they found their word meanings. After all, the corpus accurately reflects the fact that there are more men than women who work in computer science \cite{womenincs}. So the biased outcome of the search mirrors societal structure, or so would the recommender's excuse might sound. 

A further point to consider is the fact that the nature of the object supposedly captured by DS - meaning - is highly contested. Conceptual confusion and debate abound concerning the exact relation around the concepts of meaning, groundedness, speaker and community. This can make it even harder to adequately respond to calls of responsibility in these matters because the relation that holds between corpus, DS techniques, meaning and society are far from settled. 

This story goes to show that it can be excruciatingly difficult to find the responsible human agent when DS is used for algorithmic systems. Even if one grants that the meanings as found in a corpus are biased in an unacceptable way, the question still remains what acceptability would entail. A follow to this is whether one can remove prejudice and bias from meaning spaces without rendering them unusable for usage in applications. What, for example, if the fact that the expression "woman" is functional in natural language discourse precisely because of its unfortunate semantic relationship to "man". A recent paper by Bolukbasi et al. contradicts this pessimistic view \cite{bolukbasi2016man}. According to it, it is possible to debias the meanings of a semantic space along binary gender dimensions while still preserving its useful properties.

The lesson of this particular successfull debiasing endeavor is straightforward. Performance is no general excuse for designers of systems employing DS to work with meanings that reflect societal bias. Efforts can be made to process the DS model after its original training to rid it of discriminatory disparities. In general domain ML there is already a sizable body of literature of finding an removing so-called \emph{disparate impact} - unintentional discrimination (cf. \cite{feldman2015certifying} \cite{dwork2012fairness}). 
Similar collections of standardized techniques should be developed to debias semantic spaces. Doing this is especially important, of course, if the DS model to be used for some outside application. 
Perhaps a complement to the attempts to debiase embeddings, then, are the studies whose stated goal it is to uncover biases in corpora (cf. \cite{wagner2015s} \cite{herbelot2012distributional}). Findings that indicate clear instances of bias in widely used corpora can then be used as an indication where  debiasing should have to take place before the DS is put to further use.
Thus, it appears that a two-fold effort is required in to prevent damage from happening in the employment of algorithmically generated meanings. On the one hand, one will constantly have to scan the semantic spaces gathered from corpora for undesired connotations or other semantic relationships within it; on the other, once such biases are certified, efforts need to be undertaken, so that systems using this semantic information do not systematically disadvantage people.

In this fashion fairness principle could be addressed in DS. This does not yet respond, however, to the question where exactly responsibility lies - that is, with whom - in case something does go wrong.
It is perhaps the most intricate difficulty concerning ethics in DS as potential culprits are society at large, the producers of the corpus, the distributional semanticist or the person who implements the system that interacts with society again. 
The nature of language meaning and how language and society interact are notoriously contested - the heated debates about what is appropriate in gendering language serves as a case in point. For data engineers and programmers who work with these systems there is certainly no easy answer to this issue at first sight.
The best that can be offered in terms of practical advice is that if the possibilty exists to algorithmically rid a DS model of biases relevant to its application, then this should be done.
\section{Conclusion}\hypertarget{sec5}{}

\section{References}\hypertarget{sec6}{ }
\printbibliography

\end{document}